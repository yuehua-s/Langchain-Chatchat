import uuid

import streamlit as st
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from streamlit_extras.bottom_container import bottom

from chatchat.settings import Settings
from chatchat.server.chat.graph_chat import create_agent_models

from chatchat.webui_pages.utils import *
from chatchat.webui_pages.dialogue.dialogue import list_graphs, list_tools
from chatchat.server.utils import (
    build_logger,
    get_config_models,
    get_config_platforms,
    get_default_llm,
    get_history_len,
    get_recursion_limit,
    get_graph_instance,
    get_tool,
)

logger = build_logger()


def init_conversation_id():
    # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ session_state ä¸­å­˜å‚¨äº† UUID
    if "conversation_id" not in st.session_state:
        # ç”Ÿæˆä¸€ä¸ªéšæœºçš„UUIDå¹¶å­˜å‚¨åœ¨ session_state ä¸­
        st.session_state["conversation_id"] = str(uuid.uuid4())


@st.experimental_dialog("æ¨¡å‹é…ç½®", width="large")
def llm_model_setting():
    cols = st.columns(3)
    platforms = ["æ‰€æœ‰"] + list(get_config_platforms())
    platform = cols[0].selectbox("æ¨¡å‹å¹³å°è®¾ç½®(Platform)", platforms)
    llm_models = list(
        get_config_models(
            model_type="llm", platform_name=None if platform == "æ‰€æœ‰" else platform
        )
    )
    llm_model = cols[1].selectbox("æ¨¡å‹è®¾ç½®(LLM)", llm_models)
    temperature = cols[2].slider("æ¸©åº¦è®¾ç½®(Temperature)", 0.0, 1.0, value=0.01)
    system_message = st.text_area("æŒ‡ä»¤(Prompt):")

    if st.button("OK"):
        # ä¿å­˜çŠ¶æ€åˆ° session_state
        st.session_state["platform"] = platform
        st.session_state["llm_model"] = llm_model
        st.session_state["temperature"] = temperature
        st.session_state["system_message"] = system_message
        st.rerun()


def graph_agent_page(
    api: ApiRequest,
    is_lite: bool = False,
):
    import rich  # debug

    # åˆå§‹åŒ– conversation_id
    init_conversation_id()

    # åˆå§‹åŒ– session_state ä¸­çš„é”®
    if "platform" not in st.session_state:
        st.session_state["platform"] = "æ‰€æœ‰"
    if "llm_model" not in st.session_state:
        st.session_state["llm_model"] = get_default_llm()
    if "temperature" not in st.session_state:
        st.session_state["temperature"] = 0.01
    if "system_message" not in st.session_state:
        st.session_state["system_message"] = ""

    with st.sidebar:
        tab1, = st.tabs(["å·¥å…·è®¾ç½®"])

        with tab1:
            # é€‰æ‹© langgraph æ¨¡æ¿
            graph_names = list_graphs(api) + ["None"]
            selected_graph = st.selectbox(
                "é€‰æ‹©å·¥ä½œæµ(å¿…é€‰)",
                graph_names,
                format_func=lambda x: "None" if x == "None" else x,
                key="selected_graph",
            )

            # é€‰æ‹©å·¥å…·
            tools = list_tools(api)
            tool_names = ["None"] + list(tools)
            selected_tools = st.multiselect(
                "é€‰æ‹©å·¥å…·(å¯é€‰)",
                list(tools),
                format_func=lambda x: tools[x]["title"],
                key="selected_tools",
            )
            selected_tool_configs = {
                name: tool["config"]
                for name, tool in tools.items()
                if name in selected_tools
            }

    # é€‰æ‹©çš„å·¥å…·
    selected_tools_configs = list(selected_tool_configs)

    st.title("è‡ªåª’ä½“æ–‡ç« ç”Ÿæˆ")
    with st.chat_message("assistant"):
        st.write("Hello ğŸ‘‹, æˆ‘æ˜¯è‡ªåª’ä½“æ–‡ç« ç”Ÿæˆ Agent, è¯•ç€å‘æˆ‘æé—®.")

    # chat input
    with bottom():
        cols = st.columns([1, 0.2, 15,  1])
        if cols[0].button(":gear:", help="æ¨¡å‹é…ç½®"):
            llm_model_setting()
        if cols[-1].button(":wastebasket:", help="æ¸…ç©ºå¯¹è¯"):
            st.session_state["message"] = []
            st.rerun()
        # Accept user input
        user_input = cols[2].chat_input("è¯·è¾“å…¥ä½ çš„éœ€æ±‚. å¦‚: è¯·ä½ å¸®æˆ‘ç”Ÿæˆä¸€ç¯‡è‡ªåª’ä½“æ–‡ç« .")

    # debug
    with st.status("debug info", expanded=True):
        st.write("å½“å‰ llm å¹³å°:", st.session_state["platform"])
        st.write("å½“å‰ llm æ¨¡å‹:", st.session_state["llm_model"])
        st.write("å½“å‰ llm æ¸©åº¦:", st.session_state["temperature"])
        st.write("å½“å‰ç³»ç»Ÿ prompt:", st.session_state["system_message"])
        st.write("å½“å‰ tools:", selected_tools_configs)
        st.write("å½“å‰ä¼šè¯çš„ id:", st.session_state["conversation_id"])
        if st.session_state.selected_graph == "æ–‡ç« ç”Ÿæˆ":
            st.write("å½“å‰å·¥ä½œæµ:", "article_generation")
        elif st.session_state.selected_graph == "é€šç”¨æœºå™¨äºº":
            st.write("å½“å‰å·¥ä½œæµ:", "base_graph")

    all_tools = get_tool().values()
    tools = [tool for tool in all_tools if tool.name in selected_tools_configs]
    llm_model = st.session_state["llm_model"]
    llm = create_agent_models(configs=None,
                              model=llm_model,
                              max_tokens=None,
                              temperature=st.session_state["temperature"],
                              stream=True)

    rich.print(llm)

    if st.session_state.selected_graph == "æ–‡ç« ç”Ÿæˆ":
        graph_name = "article_generation"
    else:
        graph_name = "base_graph"

    graph = get_graph_instance(
        name=graph_name,
        llm=llm,
        tools=tools,
        history_len=get_history_len(),
    )
    if not graph:
        raise ValueError(f"Graph '{graph_name}' is not registered.")

    graph_config = {
        "configurable": {
            "thread_id": st.session_state["conversation_id"]
        },
        "recursion_limit": get_recursion_limit()
    }

    rich.print(graph)

    with st.status("debug graph info", expanded=True):
        st.write("å½“å‰ graph:", graph)
        st.write("å½“å‰ graph_config:", graph_config)

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_input:
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(user_input)
        # Add user message to chat history
        st.session_state.messages.append(HumanMessage(content=user_input))

        # å¯¹æ¥ langgraph
        events = graph.stream(
            {"messages": [("user", user_input)]}, graph_config, stream_mode="updates"
        )
        if events:
            # Display assistant response in chat message container
            with st.chat_message("assistant"):
                for event in events:
                    st.markdown(event["messages"])
                    # st.json(response, expanded=True)
                # Add assistant response to chat history
                st.session_state.messages.append(AIMessage(content=event["messages"]))
